{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Exercise 1: Convolutional Neural Networks in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will implement a multilayer perceptron and a convolutional neural network in Keras. Keras is a framework for deep learning, which, in our case at least, runs on top of TensorFlow (a deep learning library made by Google). Where scikit-learn made it very easy for us to use a wide range of built-in models, Keras makes it very easy for us to implement and use neural network architectures which we want to define ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "np.random.seed(42)  # Set the global random seed to make reproducible experiments (scikit-learn also use this)\n",
    "\n",
    "# Deep learning framework\n",
    "from keras.datasets import mnist  # Load MNIST dataset\n",
    "from keras.models import Sequential  # Create models sequentially\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D  # Relevant layers\n",
    "from keras.optimizers import Adam  # Optimizer for gradient descent\n",
    "from keras.backend import clear_session  # Delete previous models\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Set the global random seed for TensorFlow to make reproducible experiments\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 MLP in Keras on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to previous exercises, we will now try to implement a multilayer perceptron in Keras to classify digits in the MNIST dataset. In this exercise, however, we only look at the full size MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (already split between train and test sets)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Set number of classes\n",
    "num_classes = 10\n",
    "\n",
    "# Flatten from samples x 28 x 28 to samples x 784 (each sample is flattened from 2D picture to a vector)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')  # Cast to float32\n",
    "X_test = X_test.astype('float32')  # Cast to float32\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define and evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a neural network with two hidden layers of 25 and 10 neurons respectively, all using the Sigmoid activation function. The final layer should be a dense layer with 'num_classes' neurons and use the 'softmax' as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()  # Delete any existing models\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 12\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (784,)\n",
    "\n",
    "# Now create the model\n",
    "# See inspiration here: https://keras.io/getting-started/sequential-model-guide/\n",
    "# ====================== YOUR CODE HERE =======================\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "# Print a summary of the defined model\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using categorical crossentropy as loss function and the Adam optimizer for gradient descent\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the loss and accuracy in history\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Investigate training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see on the plots above, we do not experience issues with overfitting, as the accuracy on the train and test sets are similar. However, we might overfit the model if we let it train for more epochs. \n",
    "\n",
    "If we would like better performance, we would need a larger model, and with the current approach, the number of parameters grows too quickly if we add extra layers. You can try this by adding a couple of extra layers, and/or by using more hidden units. Try using 2048 neurons in the hidden layers, and see how the accuracy improves, but how the training time becomes an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 CNN in Keras on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will implement a convolutional neural network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Introduction to Convolution Neural networks (CNNs)\n",
    "\n",
    "A CNN is a neural network which use different hidden layers than the ones we know from MLPs. This includes convolutional layers (often using the ReLU as activation function) and pooling layers. Below are some very brief explanations of these layers, but we refer to the [notes from the cs231n course on CNNs](http://cs231n.github.io/convolutional-networks/) for more information. \n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/cs231n/cs231n.github.io/blob/master/assets/cnn/convnet.jpeg\"><img src=\"https://github.com/cs231n/cs231n.github.io/raw/master/assets/cnn/convnet.jpeg\" width=\"80%\" style=\"max-width:80%;\"></a>\n",
    "</div>    \n",
    "<br>\n",
    "  \n",
    "#### 2.1.2 Convolutional layers\n",
    "\n",
    "The convolutional layer is the core building block of a Convolutional Network. Similarly to the fully connected layers we know from previously, it takes a weighted sum of some input and feeds it through an activation function. The difference is that the convolutional layer slides a kernel over the input image, calculating the weighted sum and the activation function as it goes along. The benefit of this is signifantly fewer parameters in the model.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*1PSMTM8Brk0hsJuF.\">\n",
    "\n",
    "#### 2.1.3 Max pooling layer \n",
    "\n",
    "Max pooling passes divides a feature map in 2x2 blocks in the height and width dimensions, and then passes the maximum value each block. This effectively discards 75% of the input feature map. \n",
    "\n",
    "  <img src=\"https://github.com/cs231n/cs231n.github.io/raw/master/assets/cnn/maxpool.jpeg\" width=\"40%\" style=\"max-width:40%;\"></a>\n",
    "  <div>\n",
    " <br>   \n",
    "<div>\n",
    "  <img src=\"https://github.com/cs231n/cs231n.github.io/raw/master/assets/cnn/pool.jpeg\" width=\"36%\" style=\"max-width:50%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (already split between train and test sets)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Input image dimensions and number of classes\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "# Pre-process data\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define and evaluate the model\n",
    "Evaluate a neural network with the following architecture:\n",
    "* Convolutional layer with 32 kernels, kernel size of (3, 3), and using ReLU as activation function\n",
    "* Convolutional layer with 64 kernels, kernel size of (3, 3), and using ReLU as activation function\n",
    "* Max Pooling layer\n",
    "* Flatten layers which flattens the input feature map to a vector\n",
    "* A dense (fully connected) layer with 128 hidden units using the ReLU activation function\n",
    "* A dense layer with with 'num_classes' neurons using the 'softmax' activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()  # Delete any existing models\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Now create the model\n",
    "# See inspiration here: https://keras.io/getting-started/sequential-model-guide/\n",
    "# ====================== YOUR CODE HERE =======================\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================\n",
    "\n",
    "# Print a summary of the defined model\n",
    "model.summary()\n",
    "\n",
    "# Compile the model using categorical crossentropy as loss function and the Adam optimizer for gradient descent\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the loss and accuracy in history\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see very good performance, achieved after only a few epochs. Note that GPUs can easily reduce the training time by a factor 20-50, and sometimes more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Investigate training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not pay much attention to regularization during this exercise. Instead of using L2 regularization, we often tend to use dropout and batch normalization in CNNs. Try adding a dropout layer with a dropout rate of 0.5 just before the last dense layer, and see if this alleviates overfitting issues.\n",
    "\n",
    "To save time, we will also skip some relevant subjects. The initialization of parameters in the individual layers used to have high significance, but today we can simply use the default ones defined in Keras. The Adam optimizer can be used in most cases, so we will no pay much attention on the choice of optimizer. If you have issues with it not converging, then try setting a lower learning rate (e.g. 1e-5). The pre-processing of the data can simply be made such that it is scaled to fit between 0 and 1. Although this is not ideal given the initilization scheme, the difference in performance is so small that it rarely matters in practice. This is particularly true if you use batch normalization, which we will talk about next time. Padding was also quickly skipped, as Keras generally takes care of this too. If you would like an in-depth understanding of how CNNs work, including the concepts mentioned above, then solve Assignment 2 of the cs231n Stanford Course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualize learned filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access the learned filters in the convolutional layers. Let us try to visualize the 32 learned filters in the first layer. Note that white is high values and black is low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(32):\n",
    "    plt.subplot(6, 6, i+1)\n",
    "    plt.imshow(model.layers[0].get_weights()[0][:, :, :, i].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Filter no.: \" + str(i + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These filters are diffictult to interpret. However, take a look a the learned filters on RGB imagery below by Krizhevsky et al. Each of the 96 filters is of size 11x11x3. As these filters are convolved with input image, they detect edges and color blobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/cs231n/cs231n.github.io/raw/master/assets/cnn/weights.jpeg\" style=\"max-width:100%;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
